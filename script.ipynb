{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelo RF para evaluar distinción de sinteticos/reales, con agrupación de diagnosticos/tratamientos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import base64\n",
    "from collections import Counter\n",
    "from datetime import datetime, timedelta\n",
    "from random import choices, randint\n",
    "from faker import Faker\n",
    "import time\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "# Definición de parámetros del grid search \n",
    "frequency_values = [0.0001] # Frecuencia mínima para hacer el OHE en el set de datos que le pasamos al cluster \n",
    "k_values = [50, 150, 200] # Numero de clusters\n",
    "n_values = [50, 200] # N valores enteros\n",
    "ruta_csv_origen = \"\" # Path a archivo CMBD en csv\n",
    "results = []\n",
    "\n",
    "# Funciones auxiliares\n",
    "def generate_base64_id():\n",
    "    random_bytes = os.urandom(9)\n",
    "    base64_id = base64.b64encode(random_bytes).decode('utf-8').rstrip('=')\n",
    "    return base64_id\n",
    "\n",
    "def get_value_from_distribution(distribution):\n",
    "    population = list(distribution.keys())\n",
    "    weights = list(distribution.values())\n",
    "    return choices(population, weights, k=1)[0]\n",
    "\n",
    "def assign_diagnoses_or_procedures(distribution, max_assignments):\n",
    "    assigned_values = []\n",
    "    if 'missing' in distribution:\n",
    "        missing_probability = distribution['missing']\n",
    "        if np.random.rand() < missing_probability:\n",
    "            assigned_values = [np.nan] * max_assignments\n",
    "        else:\n",
    "            non_missing_distribution = {key: distribution[key] for key in distribution if key != 'missing'}\n",
    "            num_assignments = max(1, int((1 - missing_probability) * max_assignments))\n",
    "            assigned_values = choices(\n",
    "                list(non_missing_distribution.keys()),\n",
    "                weights=list(non_missing_distribution.values()),\n",
    "                k=num_assignments\n",
    "            )\n",
    "    else:\n",
    "        non_missing_distribution = distribution\n",
    "        num_assignments = max_assignments\n",
    "        assigned_values = choices(\n",
    "            list(non_missing_distribution.keys()),\n",
    "            weights=list(non_missing_distribution.values()),\n",
    "            k=num_assignments\n",
    "        )\n",
    "    assigned_values.sort(key=lambda x: non_missing_distribution.get(x, 0), reverse=True)\n",
    "    assigned_values += [np.nan] * (max_assignments - len(assigned_values))\n",
    "    return assigned_values\n",
    "\n",
    "def process_diagnosis_for_poa(diagnosis):\n",
    "    if isinstance(diagnosis, str):\n",
    "        if diagnosis.endswith('-S'):\n",
    "            return diagnosis[:-2], 'S'\n",
    "        elif diagnosis.endswith('-N'):\n",
    "            return diagnosis[:-2], 'N'\n",
    "        elif diagnosis.endswith('-D'):\n",
    "            return diagnosis[:-2], 'D'\n",
    "        elif diagnosis.endswith('-I'):\n",
    "            return diagnosis[:-2], 'I'\n",
    "        elif diagnosis.endswith('-E'):\n",
    "            return diagnosis[:-2], 'E'\n",
    "        return diagnosis, \"\"\n",
    "    else:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "def assign_diagnosis_with_poa_and_missing(metrics, diagnosis_type='DP'):\n",
    "    if diagnosis_type == 'DP':\n",
    "        diagnosis_main = assign_diagnoses_or_procedures(metrics.get('top_n_diagnoses_separated', {}).get('DP', {}), 1)\n",
    "        diagnosis, poa = process_diagnosis_for_poa(diagnosis_main[0])\n",
    "        return diagnosis, poa\n",
    "    else:\n",
    "        secondary_diagnoses = assign_diagnoses_or_procedures(metrics.get('top_n_diagnoses_separated', {}).get('other_diagnoses', {}), 14)\n",
    "        missing_secondary_probability = metrics.get('top_n_diagnoses_separated', {}).get('missing_secondary', 0)\n",
    "        if np.random.rand() < missing_secondary_probability:\n",
    "            secondary_diagnoses = [np.nan] * 14\n",
    "        diagnoses_with_poa = [process_diagnosis_for_poa(diag) for diag in secondary_diagnoses]\n",
    "        return diagnoses_with_poa\n",
    "\n",
    "def assign_treatments_with_missing(metrics, treatment_type='PP'):\n",
    "    if treatment_type == 'PP':\n",
    "        treatment_main = assign_diagnoses_or_procedures(metrics.get('top_n_treatments_separated', {}).get('PP', {}), 1)\n",
    "        return treatment_main[0]\n",
    "    else:\n",
    "        treatments_secondary = assign_diagnoses_or_procedures(metrics.get('top_n_treatments_separated', {}).get('other_treatments', {}), 14)\n",
    "        missing_secondary_probability = metrics.get('top_n_treatments_separated', {}).get('missing_secondary', 0)\n",
    "        if np.random.rand() < missing_secondary_probability:\n",
    "            treatments_secondary = [np.nan] * 14\n",
    "        return treatments_secondary\n",
    "\n",
    "def get_custom_binned_distribution(df, column, num_bins=20):\n",
    "    col_range = df[column].max() - df[column].min()\n",
    "    if col_range > 400:\n",
    "        num_bins = 1000\n",
    "    binned_series = pd.cut(df[column], bins=num_bins, include_lowest=True)\n",
    "    binned_distribution = binned_series.value_counts(normalize=True, sort=False).to_dict()\n",
    "    return binned_distribution\n",
    "\n",
    "def calculate_missing_metrics(df, columns):\n",
    "    df = df.copy()  # Evitar SettingWithCopyWarning\n",
    "    df['missing'] = df[columns].apply(lambda row: row.isna().all() or (row == '').all(), axis=1)\n",
    "    return df['missing'].mean()\n",
    "\n",
    "def generate_synthetic_episodes(cluster_id, num_episodes, metrics):\n",
    "    episodes = []\n",
    "    for _ in range(num_episodes):\n",
    "        patient_id = generate_base64_id()\n",
    "        patient_episodes = int(get_value_from_distribution(metrics['episode_distribution']))\n",
    "        for _ in range(patient_episodes):\n",
    "            edad_bin = get_value_from_distribution(metrics['Edat'])\n",
    "            if isinstance(edad_bin, pd.Interval):\n",
    "                edad = randint(int(edad_bin.left), int(edad_bin.right))\n",
    "            else:\n",
    "                edad = int(edad_bin)\n",
    "\n",
    "            dies_estada_bin = get_value_from_distribution(metrics['Dies_estada'])\n",
    "            if isinstance(dies_estada_bin, pd.Interval):\n",
    "                dies_estada = randint(int(dies_estada_bin.left), int(dies_estada_bin.right))\n",
    "            else:\n",
    "                dies_estada = int(dies_estada_bin)\n",
    "            dies_estada = max(0, dies_estada)\n",
    "\n",
    "            DRG_bin = get_value_from_distribution(metrics['DRG'])\n",
    "            if isinstance(DRG_bin, pd.Interval):\n",
    "                DRG = randint(int(DRG_bin.left), int(DRG_bin.right))\n",
    "            else:\n",
    "                DRG = int(DRG_bin)\n",
    "\n",
    "            current_year = datetime.now().year\n",
    "            birth_year = current_year - edad\n",
    "            birth_month_day = fake.date_of_birth(minimum_age=edad, maximum_age=edad).strftime('%m-%d')\n",
    "            data_naix = f'{birth_year}-{birth_month_day} 0:00'\n",
    "\n",
    "            data_ingres = fake.date_time_this_decade(before_now=True, after_now=False)\n",
    "            data_alta = data_ingres + timedelta(days=dies_estada)\n",
    "\n",
    "            data_ingres_str = data_ingres.strftime('%d/%m/%Y %H:%M')\n",
    "            data_alta_str = data_alta.strftime('%d/%m/%Y %H:%M')\n",
    "\n",
    "            DP, POAP = assign_diagnosis_with_poa_and_missing(metrics, diagnosis_type='DP')\n",
    "            secondary_diagnoses_with_poa = assign_diagnosis_with_poa_and_missing(metrics, diagnosis_type='other_diagnoses')\n",
    "\n",
    "            DS_list = [f'DS{i+1}' for i in range(14)]\n",
    "            POA_list = [f'POA{i+1}' for i in range(14)]\n",
    "\n",
    "            diagnosis_secondary_dict = {DS_list[i]: diag[0] for i, diag in enumerate(secondary_diagnoses_with_poa)}\n",
    "            poa_secondary_dict = {POA_list[i]: diag[1] for i, diag in enumerate(secondary_diagnoses_with_poa)}\n",
    "\n",
    "            PP = assign_treatments_with_missing(metrics, treatment_type='PP')\n",
    "            secondary_treatments = assign_treatments_with_missing(metrics, treatment_type='other_treatments')\n",
    "\n",
    "            PS_list = [f'PS{i+1}' for i in range(14)]\n",
    "            procedures_secondary_dict = {PS_list[i]: secondary_treatments[i] for i in range(14)}\n",
    "\n",
    "            record = {\n",
    "                \"Id_pacient\": patient_id,\n",
    "                \"Data_naix\": data_naix,\n",
    "                \"Edat\": edad,\n",
    "                \"Sexe\": get_value_from_distribution(metrics['Sexe']),\n",
    "                \"Data_ingres\": data_ingres_str,\n",
    "                \"Circ_admiss\": get_value_from_distribution(metrics['Circ_admiss']),\n",
    "                \"Procedencia ingres\": get_value_from_distribution(metrics['Procedencia ingres']),\n",
    "                \"Data_alta\": data_alta_str,\n",
    "                \"Circ_alta\": get_value_from_distribution(metrics['Circ_alta']),\n",
    "                \"Dies_estada\": dies_estada,\n",
    "                \"Servei_alta\": get_value_from_distribution(metrics['Servei_alta']),\n",
    "                \"Descr_Servei-alta\": get_value_from_distribution(metrics['Descr_Servei-alta']),\n",
    "                \"DRG\": DRG,\n",
    "                \"T. asist.\": get_value_from_distribution(metrics['T. asist.']),\n",
    "                \"DP\": DP, \"PP\": PP, \"POAP\": POAP, **diagnosis_secondary_dict,  **procedures_secondary_dict, **poa_secondary_dict,\n",
    "                \"cluster\": cluster_id\n",
    "            }\n",
    "            episodes.append(record)\n",
    "    return episodes\n",
    "\n",
    "# Inicio del bucle principal\n",
    "for frequency in frequency_values:\n",
    "    start_time = time.time()\n",
    "    # Cargar y procesar los datos\n",
    "    df = pd.read_csv(ruta_csv_origen, sep=';', low_memory=False)\n",
    "    df['Data_naix'] = pd.to_datetime(df['Data_naix'], dayfirst=True).dt.date\n",
    "    df['Id_pacient'] = df.groupby(['Data_naix', 'Sexe'])['Id_pacient'].transform('first')\n",
    "    output_path_modified = 'modified_CMBDAH_2021_23_episodes.csv'\n",
    "    df.to_csv(output_path_modified,  sep=';', index=False)\n",
    "\n",
    "    # Modificar diagnósticos y tratamientos\n",
    "    df['DP'] = df.apply(lambda row: f\"{row['DP']}-S\" if row['POAP'] == 'S' else (f\"{row['DP']}-N\" if row['POAP'] == 'N' else None), axis=1)\n",
    "    for i in range(1, 15):\n",
    "        df[f'DS{i}'] = df.apply(lambda row: f\"{row[f'DS{i}']}-S\" if row[f'POA{i}'] == 'S' else (f\"{row[f'DS{i}']}-N\" if row[f'POA{i}'] == 'N' else None), axis=1)\n",
    "\n",
    "    df['other_diagnostics'] = df[[f'DS{i}' for i in range(1, 15)]].apply(lambda x: [i for i in x if pd.notnull(i)], axis=1)\n",
    "    df_grouped_diagnostics = df.groupby('Id_pacient').agg({\n",
    "        'DP': lambda x: pd.Series.mode(x)[0] if not x.isna().all() else None,\n",
    "        'other_diagnostics': lambda x: [item for sublist in x for item in sublist]\n",
    "    }).reset_index()\n",
    "\n",
    "    all_diagnostics = [diag for sublist in df_grouped_diagnostics['other_diagnostics'] for diag in sublist] + df_grouped_diagnostics['DP'].dropna().tolist()\n",
    "    diagnostic_counts = pd.Series(all_diagnostics).value_counts(normalize=True)\n",
    "    significant_diagnostics = diagnostic_counts[diagnostic_counts > frequency].index\n",
    "\n",
    "    df_grouped_diagnostics['DP'] = df_grouped_diagnostics['DP'].apply(lambda x: x if x in significant_diagnostics else None)\n",
    "    df_grouped_diagnostics['other_diagnostics'] = df_grouped_diagnostics['other_diagnostics'].apply(lambda x: [diag for diag in x if diag in significant_diagnostics])\n",
    "    df_grouped_diagnostics['DP'] = df_grouped_diagnostics['DP'].apply(lambda x: [x] if pd.notnull(x) else [])\n",
    "    df_grouped_diagnostics['other_diagnostics'] = df_grouped_diagnostics['other_diagnostics'].apply(lambda x: x if len(x) > 0 else ['No_Diagnosis'])\n",
    "\n",
    "    df['PP'] = df['PP'].apply(lambda x: x if pd.notnull(x) else None)\n",
    "    df['other_treatments'] = df[[f'PS{i}' for i in range(1, 15)]].apply(lambda x: [i for i in x if pd.notnull(i)], axis=1)\n",
    "    df_grouped_treatments = df.groupby('Id_pacient').agg({\n",
    "        'PP': lambda x: pd.Series.mode(x)[0] if not x.isna().all() else None,\n",
    "        'other_treatments': lambda x: [item for sublist in x for item in sublist]\n",
    "    }).reset_index()\n",
    "\n",
    "    all_treatments = [treat for sublist in df_grouped_treatments['other_treatments'] for treat in sublist] + df_grouped_treatments['PP'].dropna().tolist()\n",
    "    treatment_counts = pd.Series(all_treatments).value_counts(normalize=True)\n",
    "    significant_treatments = treatment_counts[treatment_counts > frequency].index\n",
    "\n",
    "    df_grouped_treatments['PP'] = df_grouped_treatments['PP'].apply(lambda x: x if x in significant_treatments else None)\n",
    "    df_grouped_treatments['other_treatments'] = df_grouped_treatments['other_treatments'].apply(lambda x: [treat for treat in x if treat in significant_treatments])\n",
    "    df_grouped_treatments['PP'] = df_grouped_treatments['PP'].apply(lambda x: [x] if pd.notnull(x) else [])\n",
    "    df_grouped_treatments['other_treatments'] = df_grouped_treatments['other_treatments'].apply(lambda x: x if len(x) > 0 else ['No_Treatment'])\n",
    "\n",
    "    # Asegurarse de que no hay valores None o listas vacías\n",
    "    df_grouped_diagnostics['DP'] = df_grouped_diagnostics['DP'].apply(lambda x: x if x else ['Unknown'])\n",
    "    df_grouped_diagnostics['other_diagnostics'] = df_grouped_diagnostics['other_diagnostics'].apply(lambda x: x if x else ['Unknown'])\n",
    "\n",
    "    df_grouped_treatments['PP'] = df_grouped_treatments['PP'].apply(lambda x: x if x else ['Unknown'])\n",
    "    df_grouped_treatments['other_treatments'] = df_grouped_treatments['other_treatments'].apply(lambda x: x if x else ['Unknown'])\n",
    "\n",
    "    # Aplicar MultiLabelBinarizer\n",
    "    mlb_diagnostics = MultiLabelBinarizer()\n",
    "    diagnostics_encoded = mlb_diagnostics.fit_transform(df_grouped_diagnostics['DP'] + df_grouped_diagnostics['other_diagnostics'])\n",
    "    diagnostics_df = pd.DataFrame(diagnostics_encoded, columns=mlb_diagnostics.classes_)\n",
    "\n",
    "    mlb_treatments = MultiLabelBinarizer()\n",
    "    treatments_encoded = mlb_treatments.fit_transform(df_grouped_treatments['PP'] + df_grouped_treatments['other_treatments'])\n",
    "    treatments_df = pd.DataFrame(treatments_encoded, columns=mlb_treatments.classes_)\n",
    "\n",
    "    df_combined = pd.concat([df_grouped_diagnostics['Id_pacient'], diagnostics_df, treatments_df], axis=1)\n",
    "    output_path = 'final_ohe_patients.csv'\n",
    "    #df_combined.to_csv(output_path, sep=';', index=False)\n",
    "\n",
    "    diagnostics_encoded = diagnostics_df.values.astype(np.float64)\n",
    "    treatments_encoded = treatments_df.values.astype(np.float64)\n",
    "    X_clustering = np.hstack((diagnostics_encoded, treatments_encoded))\n",
    "\n",
    "    for k in k_values:\n",
    "        # Clustering\n",
    "        import CoreFunctions as cf\n",
    "\n",
    "        # print(f\"Combinación actual: frequency = {frequency}, k = {k}\")\n",
    "        # print(\"Muestra de X_clustering antes del clustering:\")\n",
    "        # print(X_clustering[:5])\n",
    "        # print(f\"Tipo de datos de X_clustering: {X_clustering.dtype}\")\n",
    "\n",
    "        if X_clustering.dtype == 'O':\n",
    "            X_clustering = X_clustering.astype(np.float64)\n",
    "            print(\"Se ha convertido X_clustering a tipo float64.\")\n",
    "\n",
    "        M, P, CL = cf.NaiveBayesClustering(X_clustering, k)\n",
    "\n",
    "        if len(CL) == len(df_combined):\n",
    "            df_combined['cluster'] = CL\n",
    "        else:\n",
    "            raise ValueError(\"La longitud del vector de clusters no coincide con el número de pacientes agrupados.\")\n",
    "\n",
    "        df_cluster = df.merge(df_combined[['Id_pacient', 'cluster']], on='Id_pacient', how='left')\n",
    "        df_cluster.to_csv(f'data_cluster_k{k}.csv', sep=';', index=False)\n",
    "\n",
    "        clusters = df_cluster['cluster'].unique()\n",
    "        cluster_frequencies = df_cluster['cluster'].value_counts(normalize=True)\n",
    "\n",
    "        # Aquí iniciamos el bucle sobre n_values\n",
    "        for n in n_values:\n",
    "            cluster_metrics = {}\n",
    "\n",
    "            def get_top_n_diagnoses_with_poa_suffix_v2(df_subset, diagnosis_columns, poa_columns, n, separate_DP=False):\n",
    "                df_subset = df_subset.copy()\n",
    "                count_series_secondary = pd.Series(dtype=int)\n",
    "                count_series_DP = pd.Series(dtype=int)\n",
    "                missing_count_DP = 0\n",
    "                missing_count_secondary = 0\n",
    "                total_count_DP = 0\n",
    "                total_count_secondary = 0\n",
    "                poa_suffix_map = {\n",
    "                    'S': '-S',\n",
    "                    'N': '-N',\n",
    "                    'D': '-D',\n",
    "                    'I': '-I',\n",
    "                    'E': '-E'\n",
    "                }\n",
    "                if separate_DP:\n",
    "                    df_subset.loc[:, 'DP'] = df_subset.apply(lambda row: f\"{row['DP']}{poa_suffix_map.get(row['POAP'], '')}\" if pd.notnull(row['DP']) and row['DP'] != '' else None, axis=1)\n",
    "                    count_series_DP = df_subset['DP'].value_counts()\n",
    "                    total_count_DP = len(df_subset['DP'])\n",
    "                    missing_count_DP = df_subset['DP'].isnull().sum() + (df_subset['DP'] == '').sum()\n",
    "                    diagnosis_columns = diagnosis_columns[1:]\n",
    "                    poa_columns = poa_columns[1:]\n",
    "                for diag_col, poa_col in zip(diagnosis_columns, poa_columns):\n",
    "                    df_subset.loc[:, diag_col] = df_subset.apply(lambda row: f\"{row[diag_col]}{poa_suffix_map.get(row[poa_col], '')}\" if pd.notnull(row[diag_col]) and row[diag_col] != '' else None, axis=1)\n",
    "                    missing_count_secondary += df_subset[diag_col].isnull().sum() + (df_subset[diag_col] == '').sum()\n",
    "                    count_series_secondary = count_series_secondary.add(df_subset[diag_col].value_counts(), fill_value=0)\n",
    "                    total_count_secondary += len(df_subset[diag_col])\n",
    "                dp_top_n = count_series_DP.nlargest(n)\n",
    "                dp_top_n_rel = dp_top_n / dp_top_n.sum() if dp_top_n.sum() > 0 else dp_top_n\n",
    "                secondary_top_n = count_series_secondary.nlargest(n)\n",
    "                secondary_top_n_rel = secondary_top_n / secondary_top_n.sum() if secondary_top_n.sum() > 0 else secondary_top_n\n",
    "                missing_proportion_DP = missing_count_DP / total_count_DP if total_count_DP > 0 else 0\n",
    "                missing_proportion_secondary = missing_count_secondary / total_count_secondary if total_count_secondary > 0 else 0\n",
    "                results = {\n",
    "                    'DP': dp_top_n_rel.to_dict(),\n",
    "                    'other_diagnoses': secondary_top_n_rel.to_dict(),\n",
    "                    'missing_DP': missing_proportion_DP,\n",
    "                    'missing_secondary': calculate_missing_metrics(df_subset, diagnosis_columns)\n",
    "                }\n",
    "                return results\n",
    "\n",
    "            def get_top_n_treatments_with_missing_v2(df_subset, treatment_columns, n, separate_PP=False):\n",
    "                df_subset = df_subset.copy()\n",
    "                count_series_secondary = pd.Series(dtype=int)\n",
    "                count_series_PP = pd.Series(dtype=int)\n",
    "                missing_count_PP = 0\n",
    "                missing_count_secondary = 0\n",
    "                total_count_PP = 0\n",
    "                total_count_secondary = 0\n",
    "                if separate_PP:\n",
    "                    count_series_PP = df_subset['PP'].value_counts()\n",
    "                    total_count_PP = len(df_subset['PP'])\n",
    "                    missing_count_PP = df_subset['PP'].isnull().sum() + (df_subset['PP'] == '').sum()\n",
    "                    treatment_columns = treatment_columns[1:]\n",
    "                for treatment_col in treatment_columns:\n",
    "                    missing_count_secondary += df_subset[treatment_col].isnull().sum() + (df_subset[treatment_col] == '').sum()\n",
    "                    count_series_secondary = count_series_secondary.add(df_subset[treatment_col].value_counts(), fill_value=0)\n",
    "                    total_count_secondary += len(df_subset[treatment_col])\n",
    "                pp_top_n = count_series_PP.nlargest(n)\n",
    "                pp_top_n_rel = pp_top_n / pp_top_n.sum() if pp_top_n.sum() > 0 else pp_top_n\n",
    "                secondary_top_n = count_series_secondary.nlargest(n)\n",
    "                secondary_top_n_rel = secondary_top_n / secondary_top_n.sum() if secondary_top_n.sum() > 0 else secondary_top_n\n",
    "                missing_proportion_PP = missing_count_PP / total_count_PP if total_count_PP > 0 else 0\n",
    "                missing_proportion_secondary = missing_count_secondary / total_count_secondary if total_count_secondary > 0 else 0\n",
    "                results = {\n",
    "                    'PP': pp_top_n_rel.to_dict(),\n",
    "                    'other_treatments': secondary_top_n_rel.to_dict(),\n",
    "                    'missing_PP': missing_proportion_PP,\n",
    "                    'missing_secondary': calculate_missing_metrics(df_subset, treatment_columns)\n",
    "                }\n",
    "                return results\n",
    "\n",
    "            # Cálculo de métricas por cluster con el n actual\n",
    "            for cluster_id in clusters:\n",
    "                cluster_data = df_cluster[df_cluster['cluster'] == cluster_id]\n",
    "                metrics = {'frecuencia_cluster': cluster_frequencies[cluster_id]}\n",
    "                categorical_vars = ['Sexe', 'Circ_admiss', 'Procedencia ingres', 'Circ_alta', 'Servei_alta', 'Descr_Servei-alta', 'T. asist.']\n",
    "                continuous_vars = ['Edat', 'Dies_estada', 'DRG']\n",
    "                episode_counts = cluster_data['Id_pacient'].value_counts()\n",
    "                metrics['episode_distribution'] = episode_counts.value_counts(normalize=True).to_dict()\n",
    "                for var in categorical_vars:\n",
    "                    metrics[var] = cluster_data[var].value_counts(normalize=True, dropna=False).to_dict()\n",
    "                for var in continuous_vars:\n",
    "                    metrics[var] = get_custom_binned_distribution(cluster_data, var)\n",
    "                metrics['top_n_diagnoses_separated'] = get_top_n_diagnoses_with_poa_suffix_v2(\n",
    "                    cluster_data,\n",
    "                    ['DP'] + [f'DS{i}' for i in range(1, 15)],\n",
    "                    ['POAP'] + [f'POA{i}' for i in range(1, 15)],\n",
    "                    n=n, separate_DP=True\n",
    "                )\n",
    "                metrics['top_n_treatments_separated'] = get_top_n_treatments_with_missing_v2(\n",
    "                    cluster_data,\n",
    "                    ['PP'] + [f'PS{i}' for i in range(1, 15)],\n",
    "                    n=n, separate_PP=True\n",
    "                )\n",
    "                cluster_metrics[cluster_id] = metrics\n",
    "\n",
    "            # Generación de episodios sintéticos\n",
    "            total_patients = 60000\n",
    "            episodes = []\n",
    "            for cluster_id, metrics in cluster_metrics.items():\n",
    "                cluster_frequency = metrics['frecuencia_cluster']\n",
    "                num_episodes = int(cluster_frequency * total_patients)\n",
    "                episodes += generate_synthetic_episodes(cluster_id, num_episodes, metrics)\n",
    "            df_episodes = pd.DataFrame(episodes)\n",
    "            df_episodes.to_csv(f'episodios_sinteticos_k{k}_n{n}_freq{frequency}.csv', sep=';', index=False)\n",
    "\n",
    "            # Evaluación del modelo\n",
    "            synthetic_sample = df_episodes.sample(n=1000, random_state=47).copy()\n",
    "            synthetic_sample['source'] = 'synthetic'\n",
    "            real_sample = df_cluster.sample(n=1000, random_state=49).copy()\n",
    "            real_sample['source'] = 'real'\n",
    "\n",
    "            # Preparar los datos para el modelo RandomForest\n",
    "            common_columns = synthetic_sample.columns.intersection(real_sample.columns)\n",
    "            synthetic_sample = synthetic_sample[common_columns]\n",
    "            real_sample = real_sample[common_columns]\n",
    "\n",
    "            diagnosis_columns = [f'DS{i}' for i in range(1, 15)]\n",
    "            treatment_columns = [f'PS{i}' for i in range(1, 15)]\n",
    "\n",
    "            synthetic_sample['other_diagnostics'] = synthetic_sample[diagnosis_columns].apply(lambda row: ','.join(row.dropna().astype(str)) if row.notna().any() else np.nan, axis=1)\n",
    "            real_sample['other_diagnostics'] = real_sample[diagnosis_columns].apply(lambda row: ','.join(row.dropna().astype(str)) if row.notna().any() else np.nan, axis=1)\n",
    "\n",
    "            synthetic_sample['other_treatments'] = synthetic_sample[treatment_columns].apply(lambda row: ','.join(row.dropna().astype(str)) if row.notna().any() else np.nan, axis=1)\n",
    "            real_sample['other_treatments'] = real_sample[treatment_columns].apply(lambda row: ','.join(row.dropna().astype(str)) if row.notna().any() else np.nan, axis=1)\n",
    "\n",
    "            synthetic_sample.drop(columns=diagnosis_columns + treatment_columns, inplace=True)\n",
    "            real_sample.drop(columns=diagnosis_columns + treatment_columns, inplace=True)\n",
    "\n",
    "            poa_columns = [col for col in synthetic_sample.columns if col.startswith('POA')]\n",
    "            synthetic_sample.drop(columns=poa_columns, inplace=True, errors='ignore')\n",
    "            real_sample.drop(columns=poa_columns, inplace=True, errors='ignore')\n",
    "\n",
    "            combined_data = pd.concat([synthetic_sample, real_sample], ignore_index=True)\n",
    "            combined_data['is_real'] = combined_data['source'].map({'real': 1, 'synthetic': 0})\n",
    "\n",
    "            combined_data.drop(columns=['source', 'Id_pacient'], inplace=True, errors='ignore')\n",
    "            combined_data.drop(columns=['Data_naix', 'Data_ingres', 'Data_alta', 'cluster'], inplace=True, errors='ignore')\n",
    "\n",
    "            combined_data.fillna(-1, inplace=True)\n",
    "\n",
    "            encoded_data = pd.get_dummies(combined_data)\n",
    "\n",
    "            # Usamos 'X_eval' y 'y_eval' para evitar sobrescribir 'X'\n",
    "            X_eval = encoded_data.drop(columns=['is_real'])\n",
    "            y_eval = encoded_data['is_real']\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X_eval, y_eval, test_size=0.3, random_state=42)\n",
    "            model = RandomForestClassifier(random_state=42)\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            report = classification_report(y_test, y_pred, output_dict=True)\n",
    "            precision = report['weighted avg']['precision']\n",
    "            recall = report['weighted avg']['recall']\n",
    "            f1_score = report['weighted avg']['f1-score']\n",
    "            accuracy = report['accuracy']\n",
    "            end_time = time.time()\n",
    "            execution_time = end_time - start_time\n",
    "            results.append({\n",
    "                'frequency': frequency,\n",
    "                'k': k,\n",
    "                'n': n,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1_score': f1_score,\n",
    "                'accuracy': accuracy,\n",
    "                'execution_time': execution_time\n",
    "            })\n",
    "\n",
    "# Encontrar el mejor resultado\n",
    "results_df = pd.DataFrame(results)\n",
    "best_iteration = results_df.loc[results_df['f1_score'].idxmax()]\n",
    "best_frequency = best_iteration['frequency']\n",
    "best_k = best_iteration['k']\n",
    "best_n = best_iteration['n']\n",
    "\n",
    "print(\"Mejores parámetros encontrados:\")\n",
    "print(f\"Frequency: {best_frequency}, k: {best_k}, n: {best_n}\")\n",
    "print(f\"F1-Score: {best_iteration['f1_score']}, Precision: {best_iteration['precision']}, Recall: {best_iteration['recall']}\")\n",
    "print(f\"Tiempo de ejecución: {best_iteration['execution_time']} segundos\")\n",
    "\n",
    "# Mostrar el DataFrame de resultados con tiempos de ejecución\n",
    "print(\"\\nResultados completos:\")\n",
    "print(results_df)\n",
    "\n",
    "# Cargar el mejor archivo y graficar (opcional)\n",
    "# df_best = pd.read_csv(f'episodios_sinteticos_k{best_k}_n{best_n}_freq{best_frequency}.csv', sep=';')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demográficos + visitas + diagnósticos + tratamientos ->\t\t82%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Cargar los datos de los episodios sintéticos y los episodios modificados del dataset real\n",
    "synthetic_data = pd.read_csv(f'episodios_sinteticos_k{best_k}_n{best_n}_freq{best_frequency}.csv', sep=';')\n",
    "real_data = pd.read_csv(f'data_cluster_k{best_k}.csv', sep=';')\n",
    "\n",
    "# Seleccionar 1000 muestras aleatorias de los datos sintéticos y reales\n",
    "synthetic_sample = synthetic_data.sample(n=1000, random_state=42).copy()\n",
    "synthetic_sample['source'] = 'synthetic'  # Marcar los episodios sintéticos\n",
    "\n",
    "real_sample = real_data.sample(n=1000, random_state=42).copy()\n",
    "real_sample['source'] = 'real'  # Marcar los episodios reales\n",
    "\n",
    "# Determinar columnas comunes entre los dos conjuntos de datos\n",
    "common_columns = synthetic_sample.columns.intersection(real_sample.columns)\n",
    "\n",
    "# Filtrar ambos conjuntos de datos para que solo contengan columnas comunes\n",
    "synthetic_sample = synthetic_sample[common_columns]\n",
    "real_sample = real_sample[common_columns]\n",
    "\n",
    "# Verificación de las formas de los dataframes después de filtrar\n",
    "print(f\"Forma de la muestra sintética filtrada: {synthetic_sample.shape}\")\n",
    "print(f\"Forma de la muestra real filtrada: {real_sample.shape}\")\n",
    "\n",
    "# Agrupar las columnas de diagnósticos DS01, DS02... en una sola columna 'other_diagnostics'\n",
    "diagnosis_columns = [col for col in synthetic_sample.columns if col.startswith('DS')]\n",
    "synthetic_sample['other_diagnostics'] = synthetic_sample[diagnosis_columns].apply(lambda row: ','.join(row.dropna().astype(str)) if row.notna().any() else np.nan, axis=1)\n",
    "real_sample['other_diagnostics'] = real_sample[diagnosis_columns].apply(lambda row: ','.join(row.dropna().astype(str)) if row.notna().any() else np.nan, axis=1)\n",
    "\n",
    "# Agrupar las columnas de tratamientos PS01, PS02... en una sola columna 'other_treatments'\n",
    "treatment_columns = [col for col in synthetic_sample.columns if col.startswith('PS')]\n",
    "synthetic_sample['other_treatments'] = synthetic_sample[treatment_columns].apply(lambda row: ','.join(row.dropna().astype(str)) if row.notna().any() else np.nan, axis=1)\n",
    "real_sample['other_treatments'] = real_sample[treatment_columns].apply(lambda row: ','.join(row.dropna().astype(str)) if row.notna().any() else np.nan, axis=1)\n",
    "\n",
    "# Eliminar las columnas de diagnósticos, tratamientos y POA ya que las hemos agrupado\n",
    "synthetic_sample.drop(columns=diagnosis_columns + treatment_columns, inplace=True)\n",
    "real_sample.drop(columns=diagnosis_columns + treatment_columns, inplace=True)\n",
    "\n",
    "# Eliminar las columnas de POA (POA1, POA2...) ya que no las queremos considerar\n",
    "poa_columns = [col for col in synthetic_sample.columns if col.startswith('POA')]\n",
    "synthetic_sample.drop(columns=poa_columns, inplace=True, errors='ignore')\n",
    "real_sample.drop(columns=poa_columns, inplace=True, errors='ignore')\n",
    "\n",
    "# Combinar los datos sintéticos y reales en un único DataFrame\n",
    "combined_data = pd.concat([synthetic_sample, real_sample], ignore_index=True)\n",
    "\n",
    "# Crear columna objetivo: 1 para datos reales, 0 para sintéticos\n",
    "combined_data['is_real'] = combined_data['source'].map({'real': 1, 'synthetic': 0})\n",
    "\n",
    "# Eliminar columnas innecesarias como 'source' y 'Id_pacient'\n",
    "combined_data.drop(columns=['source', 'Id_pacient'], inplace=True, errors='ignore')\n",
    "\n",
    "# Eliminar también las columnas de fechas\n",
    "combined_data.drop(columns=['Data_naix', 'Data_ingres', 'Data_alta'], inplace=True, errors='ignore')\n",
    "\n",
    "# Imputar valores faltantes con un valor placeholder (por ejemplo, -1)\n",
    "combined_data.fillna(-1, inplace=True)\n",
    "\n",
    "# Convertir variables categóricas a variables dummy\n",
    "encoded_data = pd.get_dummies(combined_data)\n",
    "\n",
    "# Definir las características y la variable objetivo\n",
    "X = encoded_data.drop(columns=['is_real'])\n",
    "y = encoded_data['is_real']\n",
    "\n",
    "# División de los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=48)\n",
    "\n",
    "# Inicializar y entrenar el modelo RandomForest\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones con el modelo entrenado\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluación del modelo\n",
    "print(\"Matriz de Confusión:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nInforme de Clasificación:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Obtener la importancia de las características\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Característica': X_train.columns,\n",
    "    'Importancia': model.feature_importances_\n",
    "}).sort_values(by='Importancia', ascending=False)\n",
    "\n",
    "# Mostrar las 20 características más importantes\n",
    "print(\"Top 20 Características más Importantes:\")\n",
    "print(feature_importance.head(20))\n",
    "\n",
    "# Visualización de la importancia de las características\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='Importancia', y='Característica', data=feature_importance.head(20))\n",
    "plt.title('Top 20 Características más Importantes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Completo (Anterior + POA) ->\t\t\t93% \tEl orden importa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Cargar los datos de los episodios sintéticos y los episodios modificados del dataset real\n",
    "synthetic_data = pd.read_csv(f'episodios_sinteticos_k{best_k}_n{best_n}_freq{best_frequency}.csv', sep=';')\n",
    "real_data = pd.read_csv(f'data_cluster_k{best_k}.csv', sep=';')\n",
    "\n",
    "# Seleccionar 1000 muestras aleatorias de los datos sintéticos y reales\n",
    "synthetic_sample = synthetic_data.sample(n=1000, random_state=42).copy()\n",
    "synthetic_sample['source'] = 'synthetic'  # Marcar los episodios sintéticos\n",
    "\n",
    "real_sample = real_data.sample(n=1000, random_state=42).copy()\n",
    "real_sample['source'] = 'real'  # Marcar los episodios reales\n",
    "\n",
    "# Determinar columnas comunes entre los dos conjuntos de datos\n",
    "common_columns = synthetic_sample.columns.intersection(real_sample.columns)\n",
    "\n",
    "# Filtrar ambos conjuntos de datos para que solo contengan columnas comunes\n",
    "synthetic_sample = synthetic_sample[common_columns]\n",
    "real_sample = real_sample[common_columns]\n",
    "\n",
    "# Verificación de las formas de los dataframes después de filtrar\n",
    "print(f\"Forma de la muestra sintética filtrada: {synthetic_sample.shape}\")\n",
    "print(f\"Forma de la muestra real filtrada: {real_sample.shape}\")\n",
    "\n",
    "# Agrupar las columnas de diagnósticos DS01, DS02... en una sola columna 'other_diagnostics'\n",
    "diagnosis_columns = [col for col in synthetic_sample.columns if col.startswith('DS')]\n",
    "synthetic_sample['other_diagnostics'] = synthetic_sample[diagnosis_columns].apply(lambda row: ','.join(row.dropna().astype(str)), axis=1)\n",
    "real_sample['other_diagnostics'] = real_sample[diagnosis_columns].apply(lambda row: ','.join(row.dropna().astype(str)), axis=1)\n",
    "\n",
    "# Agrupar las columnas de tratamientos PS01, PS02... en una sola columna 'other_treatments'\n",
    "treatment_columns = [col for col in synthetic_sample.columns if col.startswith('PS')]\n",
    "synthetic_sample['other_treatments'] = synthetic_sample[treatment_columns].apply(lambda row: ','.join(row.dropna().astype(str)), axis=1)\n",
    "real_sample['other_treatments'] = real_sample[treatment_columns].apply(lambda row: ','.join(row.dropna().astype(str)), axis=1)\n",
    "\n",
    "# Eliminar las columnas de diagnósticos, tratamientos y POA ya que las hemos agrupado\n",
    "synthetic_sample.drop(columns=diagnosis_columns + treatment_columns, inplace=True)\n",
    "real_sample.drop(columns=diagnosis_columns + treatment_columns, inplace=True)\n",
    "\n",
    "# Eliminar también las columnas de POA (POA1, POA2...)\n",
    "# poa_columns = [col for col in synthetic_sample.columns if col.startswith('POA')]\n",
    "# synthetic_sample.drop(columns=poa_columns, inplace=True, errors='ignore')\n",
    "# real_sample.drop(columns=poa_columns, inplace=True, errors='ignore')\n",
    "\n",
    "# Combinar los datos sintéticos y reales en un único DataFrame\n",
    "combined_data = pd.concat([synthetic_sample, real_sample], ignore_index=True)\n",
    "\n",
    "# Crear columna objetivo: 1 para datos reales, 0 para sintéticos\n",
    "combined_data['is_real'] = combined_data['source'].map({'real': 1, 'synthetic': 0})\n",
    "\n",
    "# Eliminar columnas innecesarias como 'source' y 'Id_pacient'\n",
    "combined_data.drop(columns=['source', 'Id_pacient'], inplace=True, errors='ignore')\n",
    "\n",
    "# Eliminar también las columnas de fechas\n",
    "combined_data.drop(columns=['Data_naix', 'Data_ingres', 'Data_alta'], inplace=True, errors='ignore')\n",
    "\n",
    "# Imputar valores faltantes con un valor placeholder (por ejemplo, -1)\n",
    "combined_data.fillna(-1, inplace=True)\n",
    "\n",
    "# Convertir variables categóricas a variables dummy\n",
    "encoded_data = pd.get_dummies(combined_data)\n",
    "\n",
    "# Definir las características y la variable objetivo\n",
    "X = encoded_data.drop(columns=['is_real'])\n",
    "y = encoded_data['is_real']\n",
    "\n",
    "# División de los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=48)\n",
    "\n",
    "# Inicializar y entrenar el modelo RandomForest\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones con el modelo entrenado\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluación del modelo\n",
    "print(\"Matriz de Confusión:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nInforme de Clasificación:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Obtener la importancia de las características\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Característica': X_train.columns,\n",
    "    'Importancia': model.feature_importances_\n",
    "}).sort_values(by='Importancia', ascending=False)\n",
    "\n",
    "# Mostrar las 20 características más importantes\n",
    "print(\"Top 20 Características más Importantes:\")\n",
    "print(feature_importance.head(20))\n",
    "\n",
    "# Visualización de la importancia de las características\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='Importancia', y='Característica', data=feature_importance.head(20))\n",
    "plt.title('Top 20 Características más Importantes')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Asegurarse de que las variables categóricas sean strings para Seaborn\n",
    "encoded_data = encoded_data.astype(str)\n",
    "encoded_data['is_real'] = encoded_data['is_real'].astype(str)\n",
    "\n",
    "# Obtener la importancia de las características calculadas por RandomForest\n",
    "# Suponiendo que ya tienes el DataFrame `feature_importance` calculado\n",
    "# Añadimos 'Edat' si no está presente en el top de importancia\n",
    "if 'Edat' not in feature_importance['Característica'].values:\n",
    "    edat_row = pd.DataFrame({'Característica': ['Edat'], 'Importancia': [0]})\n",
    "    feature_importance = pd.concat([feature_importance, edat_row], ignore_index=True)\n",
    "\n",
    "# Tomar las 20 características más importantes\n",
    "top_important_columns = feature_importance.head(20)['Característica'].tolist()\n",
    "\n",
    "# Identificar automáticamente las variables continuas en 'encoded_data'\n",
    "continuous_vars = encoded_data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "continuous_vars = ['Dies_estada', 'Edat', 'DRG']\n",
    "# Asegurarse de que las variables categóricas sean aquellas que no son numéricas\n",
    "categorical_vars = [col for col in top_important_columns if col not in continuous_vars]\n",
    "\n",
    "# Filtrar el conjunto de datos solo con las columnas importantes que existan\n",
    "filtered_data = encoded_data[[col for col in top_important_columns if col in encoded_data.columns] + ['is_real']]\n",
    "\n",
    "# Verificar si hay variables continuas antes de graficar\n",
    "if len(continuous_vars) > 0:\n",
    "    # Crear subplots para las variables continuas en una cuadrícula de 2 columnas\n",
    "    fig, axes = plt.subplots(nrows=(len(continuous_vars) + 1) // 2, ncols=2, figsize=(16, 8))\n",
    "\n",
    "    # Asegurar que axes es un array plano para fácil indexación si hay solo una fila\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Graficar las variables continuas\n",
    "    for i, var in enumerate(continuous_vars):\n",
    "        if var in filtered_data.columns:\n",
    "            sns.kdeplot(filtered_data[filtered_data['is_real'] == '0'][var].astype(float), ax=axes[i], label='Synthetic', shade=True)\n",
    "            sns.kdeplot(filtered_data[filtered_data['is_real'] == '1'][var].astype(float), ax=axes[i], label='Real', shade=True)\n",
    "            \n",
    "            axes[i].set_title(f'Distribución de {var}')\n",
    "            axes[i].set_xlabel(var)\n",
    "            axes[i].set_ylabel('Densidad')\n",
    "            axes[i].legend()\n",
    "\n",
    "    # Eliminar ejes no utilizados si hay más espacios en la cuadrícula de lo necesario\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No hay variables continuas para graficar.\")\n",
    "\n",
    "# Verificar si hay variables categóricas antes de graficar\n",
    "if len(categorical_vars) > 0:\n",
    "    # Crear subplots para las variables categóricas en una cuadrícula de 2 columnas\n",
    "    fig, axes = plt.subplots(nrows=(len(categorical_vars) + 1) // 2, ncols=2, figsize=(16, 24))\n",
    "\n",
    "    # Asegurar que axes es un array plano para fácil indexación si hay solo una fila\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Graficar las variables categóricas\n",
    "    for i, var in enumerate(categorical_vars):\n",
    "        if var in filtered_data.columns:\n",
    "            sns.countplot(data=filtered_data, x=var, hue='is_real', ax=axes[i])\n",
    "            axes[i].set_title(f'Frecuencia de {var}')\n",
    "            axes[i].set_xticklabels(axes[i].get_xticklabels(), rotation=90)\n",
    "            axes[i].set_xlabel(var)\n",
    "            axes[i].set_ylabel('Frecuencia')\n",
    "\n",
    "    # Eliminar ejes no utilizados si hay más espacios en la cuadrícula de lo necesario\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No hay variables categóricas para graficar.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sólo demográficos + diagnósticos ->\t\t56%\tReplicar resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Cargar los datos de los episodios sintéticos y los episodios modificados del dataset real\n",
    "synthetic_data = pd.read_csv(f'episodios_sinteticos_k{best_k}_n{best_n}_freq{best_frequency}.csv', sep=';')\n",
    "real_data = pd.read_csv(f'data_cluster_k{best_k}.csv', sep=';')\n",
    "\n",
    "# Seleccionar 1000 muestras aleatorias de los datos sintéticos y reales\n",
    "synthetic_sample = synthetic_data.sample(n=1000, random_state=42).copy()\n",
    "synthetic_sample['source'] = 'synthetic'  # Marcar los episodios sintéticos\n",
    "\n",
    "real_sample = real_data.sample(n=1000, random_state=42).copy()\n",
    "real_sample['source'] = 'real'  # Marcar los episodios reales\n",
    "\n",
    "\n",
    "# Mantener solo las columnas comunes\n",
    "common_columns = synthetic_sample.columns.intersection(real_sample.columns)\n",
    "synthetic_sample = synthetic_sample[common_columns]\n",
    "real_sample = real_sample[common_columns]\n",
    "\n",
    "# Procesar columnas de diagnósticos y tratamientos\n",
    "diagnosis_columns = [f'DS{i}' for i in range(1, 15)]\n",
    "treatment_columns = [f'PS{i}' for i in range(1, 15)]\n",
    "\n",
    "synthetic_sample['other_diagnostics'] = synthetic_sample[diagnosis_columns].apply(\n",
    "    lambda row: ','.join(row.dropna().astype(str)) if row.notna().any() else np.nan, axis=1)\n",
    "real_sample['other_diagnostics'] = real_sample[diagnosis_columns].apply(\n",
    "    lambda row: ','.join(row.dropna().astype(str)) if row.notna().any() else np.nan, axis=1)\n",
    "\n",
    "synthetic_sample['other_treatments'] = synthetic_sample[treatment_columns].apply(\n",
    "    lambda row: ','.join(row.dropna().astype(str)) if row.notna().any() else np.nan, axis=1)\n",
    "real_sample['other_treatments'] = real_sample[treatment_columns].apply(\n",
    "    lambda row: ','.join(row.dropna().astype(str)) if row.notna().any() else np.nan, axis=1)\n",
    "\n",
    "# Eliminar columnas innecesarias\n",
    "synthetic_sample.drop(columns=diagnosis_columns + treatment_columns, inplace=True)\n",
    "real_sample.drop(columns=diagnosis_columns + treatment_columns, inplace=True)\n",
    "\n",
    "# Eliminar columnas POA si están presentes\n",
    "poa_columns = [col for col in synthetic_sample.columns if col.startswith('POA')]\n",
    "synthetic_sample.drop(columns=poa_columns, inplace=True, errors='ignore')\n",
    "real_sample.drop(columns=poa_columns, inplace=True, errors='ignore')\n",
    "\n",
    "# Combinar los datos\n",
    "combined_data = pd.concat([synthetic_sample, real_sample], ignore_index=True)\n",
    "combined_data['is_real'] = combined_data['source'].map({'real': 1, 'synthetic': 0})\n",
    "\n",
    "# Eliminar columnas adicionales innecesarias\n",
    "combined_data.drop(columns=['source', 'Id_pacient'], inplace=True, errors='ignore')\n",
    "combined_data.drop(columns=['Data_naix', 'Data_ingres', 'Data_alta', 'cluster'], inplace=True, errors='ignore')\n",
    "\n",
    "# Lista de columnas a eliminar\n",
    "columns_to_drop = ['Circ_admiss', 'Procedencia ingres', 'Circ_alta', 'Servei_alta', 'Descr_Servei-alta', 'T. asist.',\n",
    "                   'Dies_estada', 'Data_ingres', 'Data_alta', 'DRG', 'PP', 'other_treatments']\n",
    "\n",
    "# Eliminar columnas no deseadas\n",
    "\n",
    "combined_data.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "\n",
    "\n",
    "# Manejar valores faltantes\n",
    "combined_data.fillna(-1, inplace=True)\n",
    "\n",
    "# Codificación one-hot\n",
    "encoded_data = pd.get_dummies(combined_data)\n",
    "\n",
    "# Paso 4: Entrenar el modelo Random Forest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Preparar características y objetivo\n",
    "X_eval = encoded_data.drop(columns=['is_real'])\n",
    "y_eval = encoded_data['is_real']\n",
    "\n",
    "# Dividir los datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_eval, y_eval, test_size=0.3, random_state=42)\n",
    "\n",
    "# Entrenar el modelo\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el modelo\n",
    "y_pred = model.predict(X_test)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "\n",
    "# Extraer importancias de las características\n",
    "importances = model.feature_importances_\n",
    "feature_names = X_eval.columns\n",
    "feature_importance = pd.DataFrame({'Característica': feature_names, 'Importancia': importances})\n",
    "feature_importance.sort_values(by='Importancia', ascending=False, inplace=True)\n",
    "feature_importance.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Paso 5: Generar los gráficos utilizando tu código\n",
    "# Asegurar que las variables categóricas sean strings para Seaborn\n",
    "encoded_data_plot = encoded_data.copy()\n",
    "encoded_data_plot['is_real'] = encoded_data_plot['is_real'].astype(str)\n",
    "\n",
    "# Tomar las 20 características más importantes\n",
    "top_important_columns = feature_importance.head(20)['Característica'].tolist()\n",
    "\n",
    "\n",
    "# Visualización de la importancia de las características\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='Importancia', y='Característica', data=feature_importance.head(20))\n",
    "plt.title('Top 20 Características más Importantes')\n",
    "plt.show()\n",
    "\n",
    "# Identificar variables continuas\n",
    "continuous_vars = ['Edat', 'Dies_estada', 'DRG']\n",
    "continuous_vars = [var for var in continuous_vars if var in encoded_data_plot.columns]\n",
    "\n",
    "# Variables categóricas son las más importantes excluyendo las continuas\n",
    "categorical_vars = [col for col in top_important_columns if col not in continuous_vars]\n",
    "\n",
    "# Filtrar el conjunto de datos solo con las columnas importantes\n",
    "filtered_data = encoded_data_plot[top_important_columns + ['is_real']]\n",
    "\n",
    "# Graficar variables continuas\n",
    "if len(continuous_vars) > 0:\n",
    "    # Crear subplots para variables continuas\n",
    "    fig, axes = plt.subplots(nrows=(len(continuous_vars) + 1) // 2, ncols=2, figsize=(16, 8))\n",
    "    axes = axes.flatten()\n",
    "    for i, var in enumerate(continuous_vars):\n",
    "        if var in filtered_data.columns:\n",
    "            sns.kdeplot(data=filtered_data[filtered_data['is_real'] == '0'], x=var, ax=axes[i], label='Synthetic', shade=True)\n",
    "            sns.kdeplot(data=filtered_data[filtered_data['is_real'] == '1'], x=var, ax=axes[i], label='Real', shade=True)\n",
    "            axes[i].set_title(f'Distribución de {var}')\n",
    "            axes[i].set_xlabel(var)\n",
    "            axes[i].set_ylabel('Densidad')\n",
    "            axes[i].legend()\n",
    "    # Eliminar ejes no utilizados\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No hay variables continuas para graficar.\")\n",
    "\n",
    "# Graficar variables categóricas\n",
    "if len(categorical_vars) > 0:\n",
    "    # Crear subplots para variables categóricas\n",
    "    fig, axes = plt.subplots(nrows=(len(categorical_vars) + 1) // 2, ncols=2, figsize=(16, 24))\n",
    "    axes = axes.flatten()\n",
    "    for i, var in enumerate(categorical_vars):\n",
    "        if var in filtered_data.columns:\n",
    "            sns.countplot(data=filtered_data, x=var, hue='is_real', ax=axes[i])\n",
    "            axes[i].set_title(f'Frecuencia de {var}')\n",
    "            axes[i].set_xticklabels(axes[i].get_xticklabels(), rotation=90)\n",
    "            axes[i].set_xlabel(var)\n",
    "            axes[i].set_ylabel('Frecuencia')\n",
    "    # Eliminar ejes no utilizados\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No hay variables categóricas para graficar.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
